# Baseline Code for DeeperAction Challenge - [MultiSports Track on Human Action Detection](https://codalab.lisn.upsaclay.fr/competitions/3736)

- [Baseline Code for DeeperAction Challenge - MultiSports Track on Human Action Detection](#baseline-code-for-deeperaction-challenge---multisports-track-on-human-action-detection)
  - [1. Competition Introduction](#1-competition-introduction)
  - [2. Download the Dataset](#2-download-the-dataset)
  - [3. Install MMAction2](#3-install-mmaction2)
  - [4. Run Baseline Experiment](#4-run-baseline-experiment)
    - [Step 1. Prepare Annotations](#step-1-prepare-annotations)
    - [Step 2. Train](#step-2-train)
    - [Step 3. Test](#step-3-test)
  - [5. Visualize Ground Truth and Prediction](#5-visualize-ground-truth-and-prediction)
  - [6. Baseline Algorithms](#6-baseline-algorithms)
    - [6.1 SlowFast-Det Baseline](#61-slowfast-det-baseline)
    - [6.2 Spatio-Temporal Action Detection Under Large Motion (1st place in ECCV'2022)](#62-spatio-temporal-action-detection-under-large-motion-1st-place-in-eccv2022)
    - [6.3 Person-Context Cross Attention for Spatio-Temporal Action Detection (1st place in ICCV'2021)](#63-person-context-cross-attention-for-spatio-temporal-action-detection-1st-place-in-iccv2021)

## 1. Competition Introduction

- Description

  The challenge is Track 2 at ECCV DeeperAction Challenge. This track is for spatio-temporal action localization within an untrimmed video. The challenge will be carried out on the MultiSports dataset. More information on the dataset can be found on MultiSports Page.

- Goal

  Given an untrimmed video, we aim at spatio-temporal action detection. Hence, participants should find the frames that contain actions, and where these actions occur

## 2. Download the Dataset

- Data and Annotations

  The data and annotations can now be downloaded from [Hugging Face](https://huggingface.co/datasets/MCG-NJU/MultiSports).

  You can still register on the [competition page](https://codalab.lisn.upsaclay.fr/competitions/3736), and download data and annotations in the Participate/Data part.

  MultiSports Dataset License: [CC BY_NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)

- Person Boxes

  MultiSports provides the **person boxes** generated by the person detector of Faster R-CNN with a ResNeXt-101-FPN backbone in the links below. We finetune the person detector only with the train set data.

  Onedrive: https://1drv.ms/f/s!AtjeLq7YnYGRe3eQMuQk5GYYu40

  Baidu Wangpan: https://pan.baidu.com/s/1zOylA-idz2foeEaU1gx6sw (password: 5ccx)

## 3. Install MMAction2

The baseline code is based on MMAction2, You can run the following commands to install MMAction2:

```shell
# prepare environment with PyTorch
conda create --name openmmlab python=3.8 -y
conda activate openmmlab
conda install pytorch torchvision -c pytorch
# install MMEngine & MMCV
pip install -U openmim
mim install mmengine 'mmcv>=2.0.0rc1'
# install MMAction2 and checkout to the 1.x branch
git clone https://github.com/open-mmlab/mmaction2.git
cd mmaction2
git checkout 1.x
pip install -v -e .
```

For more details, you can refer to the installation [document](../../docs/en/get_started.md).

## 4. Run Baseline Experiment

### Step 1. Prepare Annotations

You can run the following script to convert the original annotation to a common format in MMAction2

```python
python convert_anno.py
```

### Step 2. Train

You can use the following command to train a model.

```shell
python project/multisports/tools/train.py project/multisports/ms_lib/configs/slowfast-det_multisports.py
```

For more details, you can refer to the **Training** part in the [Training and Test Tutorial](/docs/en/user_guides/4_train_test.md).

### Step 3. Test

You can use the following command to test a model and dump the result.

```shell
python project/multisports/tools/test.py ${CONFIG_FILE} ${CHECKPOINT_FILE} --dump ${OUT_PATH}
```

We further provide a script to convert the result to the requested format of MultiSports track.

```shell
python project/multisports/tools/generate_upload_result.py ${ORI_RESULT_PATH} ${UPLOAD_RESULT_PATH}
```

## 5. Visualize Ground Truth and Prediction

We provide a [visualization tool](<>) to draw the annotation and prediction boxes on the video, which could help you browse the dataset and detailed analyze the model performance.

<div align="center">
 <img src="https://user-images.githubusercontent.com/33249023/223982910-45f97d12-6a97-45e0-a20d-00e9056c60c2.gif" width = "640" height = "360" alt="MultiSports Dataset" align=center />
</div>

## 6. Baseline Algorithms

We implement three algorithms on the project, the performance comparison is shown in the table below:

|    Algorithm    | Frame-AP@0.5 | Video-AP@0.2 | Video-AP@0.5 | Video-AP 0.1:0.9 |
| :-------------: | :----------: | :----------: | :----------: | :--------------: |
| ` SlowFast-Det` |     29.5     |     28.1     |     8.4      |       12.3       |
| `SlowFast-PCCA` |     42.2     |     41.0     |     20.0     |       20.9       |
| `SlowFast-TAAD` |     55.3     |     60.6     |     37.0     |       33.7       |

### 6.1 SlowFast-Det Baseline

Here, we briefly introduce the main modules of the `SlowFast-Det` baseline, to show you the solution of spatio-temporal action detection task in MMAction2.

The config file provides a great overview of the algorithm:

- The model we used here is `FastRCNN`, which consists of `ResNet3dSlowFast` as a backbone, naive `ROIAlign` and `BBoxHeadAVA` as a head, and `train_cfg` describes the training strategy.

- We specify the `dataset_type` as `AVADataset`, and the data augmentation is defined in train_pipeline.

- Training schedule and optimizer are defined in `param_scheduler` and `optim_wrapper`.

- More detailed introduction to the config file, please refer to the [document](../../docs/en/user_guides/1_config.md#config-system-for-spatio-temporal-action-detection)

We further provide some stronger algorithms on MultiSports, you can take them as better baselines. And what's more, they propose frequently and effectively modified modules for the task, following them, you can easily implement your idea and add them to MMAction2.

### 6.2 Spatio-Temporal Action Detection Under Large Motion (1st place in ECCV'2022)

- Introduction

  The algorithm proposes to use tube/track-aware feature aggregation modules to handle large motions, and it shows that this type of module helps in achieving great improvements over the baseline, especially for instances with such large motion

- Proposed Modules

  we briefly describe the modules as followed:

|         module name          |                      introduction                       |                           code link                           |   config   |
| :--------------------------: | :-----------------------------------------------------: | :-----------------------------------------------------------: | :--------: |
|          TOI-align           | extracts track features using ROI-Align and track boxes | [code](./lib/models/roi_heads/roi_extractors/single_toi3d.py) | [link](<>) |
| Temporal Feature Aggregation |        aggregate features across space and time         | [code](./lib/models/roi_heads/roi_extractors/single_toi3d.py) | [link](<>) |

### 6.3 Person-Context Cross Attention for Spatio-Temporal Action Detection (1st place in ICCV'2021)

- Introduction

  The solution utilizes a cross-attention mechanism to explicitly model relations between person and context for action detection.

- Proposed Modules

  we briefly describe the modules as followed:

|      module name       |                                       introduction                                       |                           code link                           |   config   |
| :--------------------: | :--------------------------------------------------------------------------------------: | :-----------------------------------------------------------: | :--------: |
|  Cross Attention Head  | cross attention transformer head for person-context modeling and human action prediction | [code](./lib/models/roi_heads/roi_extractors/single_toi3d.py) | [link](<>) |
| class balanced sampler |                decoupling strategy deals with the class imbalance problem                | [code](./lib/models/roi_heads/roi_extractors/single_toi3d.py) | [link](<>) |
