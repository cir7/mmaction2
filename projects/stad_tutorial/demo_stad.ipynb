{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/open-mmlab/mmaction2/projects/stad_tutorial/demo_stad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatio-temporal action detection with MMAction2\n",
    "Welcome to MMAction2! This is a tutorial on how to use MMAction2 for spatio-temporal action detection. In this tutorial, we will use the MultiSports dataset as an example, and provide a complete step-by-step guide for spatio-temporal action detection, including\n",
    "- Prepare spatio-temporal action detection dataset\n",
    "- Train detection model\n",
    "- Prepare AVA format dataset\n",
    "- Train spatio-temporal action detection model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install MMAction2 and MMDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U openmim\n",
    "!mim install mmengine\n",
    "!mim install mmcv\n",
    "!mim install mmdet\n",
    "\n",
    "!git clone https://github.com/open-mmlab/mmaction2.git\n",
    "\n",
    "%cd mmaction2\n",
    "%pip install -v -e .\n",
    "%cd projects/stad_tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare spatio-temporal action detection dataset\n",
    "\n",
    "Similar to detection tasks that require bounding box annotations, spatio-temporal action detection tasks require temporal and spatial localization, so more complex tube annotations are required. Taking the MultiSports dataset as an example, the `gttubes` field provides all the target action annotations in the video, and the following is an annotation fragment:\n",
    "\n",
    "```\n",
    "    'gttubes': {\n",
    "        'aerobic_gymnastics/v_aqMgwPExjD0_c001': # video_key\n",
    "            {\n",
    "                10: # label index\n",
    "                    [\n",
    "                        array([[ 377.,  904.,  316., 1016.,  584.], # 1st tube of class 10\n",
    "                               [ 378.,  882.,  315., 1016.,  579.], # shape (n, 5): n frames，each annotation includes (frame idx，x1，y1, x2, y2)\n",
    "                               ...\n",
    "                               [ 398.,  861.,  304.,  954.,  549.]], dtype=float32)，\n",
    "                        \n",
    "                        array([[ 399.,  881.,  308.,  955.,  542.], # 2nd tube of class 10\n",
    "                               [ 400.,  862.,  303.,  988.,  539.],\n",
    "                               [ 401.,  853.,  292., 1000.,  535.],\n",
    "                               ...])\n",
    "                        ...\n",
    "                                                        \n",
    "                    ] ,\n",
    "                9: # label index\n",
    "                    [\n",
    "                        array(...), # 1st tube of class 9\n",
    "                        array(...), # 2nd tube of class 9\n",
    "                        ...\n",
    "                    ]\n",
    "                ...\n",
    "            }\n",
    "    }\n",
    "```\n",
    "\n",
    "The annotation file also needs to provide other field information, and the complete ground truth file includes the following information:\n",
    "\n",
    "```\n",
    "{\n",
    "    'labels':  # label list\n",
    "        ['aerobic push up', 'aerobic explosive push up', ...],\n",
    "    'train_videos':  # training video list\n",
    "        [\n",
    "            [\n",
    "                'aerobic_gymnastics/v_aqMgwPExjD0_c001',\n",
    "                'aerobic_gymnastics/v_yaKOumdXwbU_c019',\n",
    "                ... \n",
    "            ]\n",
    "        ]\n",
    "    'test_videos':  # test video list\n",
    "        [\n",
    "            [\n",
    "                'aerobic_gymnastics/v_crsi07chcV8_c004',\n",
    "                'aerobic_gymnastics/v_dFYr67eNMwA_c005',\n",
    "                ...\n",
    "            ]\n",
    "        ]\n",
    "    'n_frames':  # dict provides frame number of each video\n",
    "        {\n",
    "            'aerobic_gymnastics/v_crsi07chcV8_c004': 725,\n",
    "            'aerobic_gymnastics/v_dFYr67eNMwA_c005': 750,\n",
    "            ...\n",
    "        }\n",
    "    'resolution':  # dict provides resolution of each video\n",
    "        {\n",
    "            'aerobic_gymnastics/v_crsi07chcV8_c004': (720, 1280),\n",
    "            'aerobic_gymnastics/v_dFYr67eNMwA_c005': (720, 1280),\n",
    "            ...\n",
    "        }\n",
    "    'gt_tubes':  # dict provides bouding boxes of each tube\n",
    "        {\n",
    "            ... # refer to above description\n",
    "        }\n",
    "}           \n",
    "```\n",
    "\n",
    "The subsequent experiments are based on MultiSports-tiny, we extracted a small number of videos from MultiSports for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "!wget -P data -c https://download.openmmlab.com/mmaction/v1.0/projects/stad_tutorial/multisports-tiny.tar\n",
    "!tar -xvf data/multisports-tiny.tar --strip 1 -C data\n",
    "!apt-get -q install tree\n",
    "!tree data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train detection model\n",
    "\n",
    "In the SlowOnly + Det paradigm, we need to train a human detector first, and then predict actions based on the detection results. In this section, we train a detection model based on the annotation format in the previous section and the MMDetection algorithm library.\n",
    "\n",
    "### 2.1 Build detection dataset annotation (COCO format)\n",
    "\n",
    "Based on the annotation information of the spatio-temporal action detection dataset, we can build a COCO format detection dataset for training the detection model. We provide a script to convert the MultiSports format annotation, if you need to convert from other formats, you can refer to the [custom dataset](https://mmdetection.readthedocs.io/zh_CN/latest/advanced_guides/customize_dataset.html) document provided by MMDetection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tools/generate_mmdet_anno.py data/multisports/annotations/multisports_GT.pkl data/multisports/annotations/multisports_det_anno.json\n",
    "!tree data/multisports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tools/generate_rgb.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Modify config file\n",
    "\n",
    "We use faster-rcnn_x101-64x4d_fpn_1x_coco as the base configuration, and make the following modifications to train on the MultiSports dataset. The following parts need to be modified:\n",
    "- Number of model categories\n",
    "- Learning rate adjustment strategy\n",
    "- Optimizer configuration\n",
    "- Dataset/annotation file path\n",
    "- Evaluator configuration\n",
    "- Pre-trained model \n",
    "  \n",
    "For more detailed tutorials, please refer to the [prepare configuration file](https://mmdetection.readthedocs.io/zh_CN/latest/user_guides/train.html#id9) document provided by MMDetection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat configs/faster-rcnn_x101-64x4d_fpn_1x_coco_ms_person.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train detection model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using MIM, you can directly train MMDetection models in the current directory. Here is the simplest example of training on a single GPU. For more training commands, please refer to the MIM [tutorial](https://github.com/open-mmlab/mim#command)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mim train mmdet configs/faster-rcnn_r50-caffe_fpn_ms-1x_coco_ms_person.py \\\n",
    "    --work-dir work_dirs/det_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Generating Proposal BBoxes\n",
    "\n",
    "During the training of the spatiotemporal action detection model, we need to rely on proposals generated by the detection model, rather than annotated detection boxes. Therefore, we need to use a trained detection model to perform inference on the entire dataset and convert the resulting proposals into the required format for subsequent training.\n",
    "\n",
    "#### 2.4.1 Converting the Dataset to Coco Format\n",
    "\n",
    "We provide a script to convert the MultiSports dataset into an annotation format without ground truth, which is used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo 'person' > data/multisports/annotations/label_map.txt\n",
    "!python tools/images2coco.py \\\n",
    "        data/multisports/rawframes \\\n",
    "        data/multisports/annotations/label_map.txt \\\n",
    "        ms_infer_anno.json "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Inference for Generating Proposal Files\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference of MMDetection models is also based on MIM. For more testing commands, please refer to the MIM [tutorial](GitHub - open-mmlab/mim: MIM Installs OpenMMLab Packages).\n",
    "\n",
    "After the inference is completed, the results will be saved in 'data/multisports/ms_proposals.pkl'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mim test mmdet configs/faster-rcnn_r50-caffe_fpn_ms-1x_coco_ms_person.py \\\n",
    "    --checkpoint work_dirs/det_model/epoch_2.pth \\\n",
    "    --out data/multisports/annotations/ms_det_proposals.pkl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Spatio-temporal Action Detection Model\n",
    "The provided annotation files and the proposal files generated by MMDetection need to be converted to the required format for training the spatiotemporal action detection model. We have provided relevant script to generate the specified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert annotation files\n",
    "!python ../../tools/data/multisports/parse_anno.py \n",
    "\n",
    "# Convert proposal files\n",
    "!python tools/convert_proposals.py \n",
    "\n",
    "!tree data/multisports/annotations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training the Spatio-temporal Action Detection Model\n",
    "\n",
    "MMAction2 already supports training on the MultiSports dataset. You just need to modify the path to the proposal file. For detailed configurations, please refer to the [config](configs/slowonly_k400_multisports.py) file. Since the training data is limited, the configuration uses a pre-trained model trained on the complete MultiSports dataset. When training with a custom dataset, you don't need to specify the `load_from` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using MIM\n",
    "!mim train mmaction2 configs/slowonly_k400_multisports.py \\\n",
    "    --work-dir work_dirs/stad_model/ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inferring the Spatiotemporal Action Detection Model\n",
    "\n",
    "After training the detection model and the spatiotemporal action detection model, we can use the spatiotemporal action detection demo for inference and visualize the model's performance.\n",
    "\n",
    "Since the tutorial uses a limited training dataset, the model's performance is not optimal, so a pre-trained model is used for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../../demo/demo_spatiotemporal_det.py \\\n",
    "    data/multisports/test/aerobic_gymnastics/v_7G_IpU0FxLU_c001.mp4 \\\n",
    "    data/demo_spatiotemporal_det.mp4 \\\n",
    "    --config configs/slowonly_k400_multisports.py \\\n",
    "    --checkpoint https://download.openmmlab.com/mmaction/v1.0/detection/slowonly/slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-8e_multisports-rgb/slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-8e_multisports-rgb_20230320-a1ca5e76.pth \\\n",
    "    --det-config configs/faster-rcnn_r50-caffe_fpn_ms-1x_coco_ms_person.py \\\n",
    "    --det-checkpoint work_dirs/det_model/epoch_2.pth \\\n",
    "    --det-score-thr 0.85 \\\n",
    "    --action-score-thr 0.8 \\\n",
    "    --label-map ../../tools/data/multisports/label_map.txt \\\n",
    "    --predict-stepsize 8 \\\n",
    "    --output-stepsize 1 \\\n",
    "    --output-fps 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Video\n",
    "import moviepy.editor\n",
    "moviepy.editor.ipython_display(\"data/demo_spatiotemporal_det.mp4\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
