{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/open-mmlab/mmaction2/projects/stad_tutorial/demo_stad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatio-temporal action detection with MMAction2\n",
    "Welcome to MMAction2! This is a tutorial on how to use MMAction2 for spatio-temporal action detection. In this tutorial, we will use the MultiSports dataset as an example, and provide a complete step-by-step guide for spatio-temporal action detection, including\n",
    "- Prepare spatio-temporal action detection dataset\n",
    "- Train detection model\n",
    "- Prepare AVA format dataset\n",
    "- Train spatio-temporal action detection model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install MMAction2 and MMDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U openmim\n",
    "!mim install mmengine\n",
    "!mim install mmcv\n",
    "!mim install mmdet\n",
    "\n",
    "!git clone https://github.com/open-mmlab/mmaction2.git\n",
    "\n",
    "%cd mmaction2\n",
    "%pip install -v -e .\n",
    "%cd projects/stad_tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare spatio-temporal action detection dataset\n",
    "\n",
    "Similar to detection tasks that require bounding box annotations, spatio-temporal action detection tasks require temporal and spatial localization, so more complex tube annotations are required. Taking the MultiSports dataset as an example, the `gttubes` field provides all the target action annotations in the video, and the following is an annotation fragment:\n",
    "\n",
    "```\n",
    "    'gttubes': {\n",
    "        'aerobic_gymnastics/v_aqMgwPExjD0_c001': # video_key\n",
    "            {\n",
    "                10: # label index\n",
    "                    [\n",
    "                        array([[ 377.,  904.,  316., 1016.,  584.], # 1st tube of class 10\n",
    "                               [ 378.,  882.,  315., 1016.,  579.], # shape (n, 5): n frames，each annotation includes (frame idx，x1，y1, x2, y2)\n",
    "                               ...\n",
    "                               [ 398.,  861.,  304.,  954.,  549.]], dtype=float32)，\n",
    "                        \n",
    "                        array([[ 399.,  881.,  308.,  955.,  542.], # 2nd tube of class 10\n",
    "                               [ 400.,  862.,  303.,  988.,  539.],\n",
    "                               [ 401.,  853.,  292., 1000.,  535.],\n",
    "                               ...])\n",
    "                        ...\n",
    "                                                        \n",
    "                    ] ,\n",
    "                9: # label index\n",
    "                    [\n",
    "                        array(...), # 1st tube of class 9\n",
    "                        array(...), # 2nd tube of class 9\n",
    "                        ...\n",
    "                    ]\n",
    "                ...\n",
    "            }\n",
    "    }\n",
    "```\n",
    "\n",
    "The annotation file also needs to provide other field information, and the complete ground truth file includes the following information:\n",
    "\n",
    "```\n",
    "{\n",
    "    'labels':  # label list\n",
    "        ['aerobic push up', 'aerobic explosive push up', ...],\n",
    "    'train_videos':  # training video list\n",
    "        [\n",
    "            [\n",
    "                'aerobic_gymnastics/v_aqMgwPExjD0_c001',\n",
    "                'aerobic_gymnastics/v_yaKOumdXwbU_c019',\n",
    "                ... \n",
    "            ]\n",
    "        ]\n",
    "    'test_videos':  # test video list\n",
    "        [\n",
    "            [\n",
    "                'aerobic_gymnastics/v_crsi07chcV8_c004',\n",
    "                'aerobic_gymnastics/v_dFYr67eNMwA_c005',\n",
    "                ...\n",
    "            ]\n",
    "        ]\n",
    "    'n_frames':  # dict provides frame number of each video\n",
    "        {\n",
    "            'aerobic_gymnastics/v_crsi07chcV8_c004': 725,\n",
    "            'aerobic_gymnastics/v_dFYr67eNMwA_c005': 750,\n",
    "            ...\n",
    "        }\n",
    "    'resolution':  # dict provides resolution of each video\n",
    "        {\n",
    "            'aerobic_gymnastics/v_crsi07chcV8_c004': (720, 1280),\n",
    "            'aerobic_gymnastics/v_dFYr67eNMwA_c005': (720, 1280),\n",
    "            ...\n",
    "        }\n",
    "    'gt_tubes':  # dict provides bouding boxes of each tube\n",
    "        {\n",
    "            ... # refer to above description\n",
    "        }\n",
    "}           \n",
    "```\n",
    "\n",
    "The subsequent experiments are based on MultiSports-tiny, we extracted a small number of videos from MultiSports for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "!wget -P data -c https://download.openmmlab.com/mmaction/v1.0/projects/stad_tutorial/multisports-tiny.tar\n",
    "!tar -xvf data/multisports-tiny.tar --strip 1 -C data\n",
    "!apt-get -q install tree\n",
    "!tree data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train detection model\n",
    "\n",
    "In the SlowOnly + Det paradigm, we need to train a human detector first, and then predict actions based on the detection results. In this section, we train a detection model based on the annotation format in the previous section and the MMDetection algorithm library.\n",
    "\n",
    "### 2.1 Build detection dataset annotation (COCO format)\n",
    "\n",
    "Based on the annotation information of the spatio-temporal action detection dataset, we can build a COCO format detection dataset for training the detection model. We provide a script to convert the MultiSports format annotation, if you need to convert from other formats, you can refer to the [custom dataset](https://mmdetection.readthedocs.io/zh_CN/latest/advanced_guides/customize_dataset.html) document provided by MMDetection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tools/generate_mmdet_anno.py data/multisports/annotations/multisports_GT.pkl data/multisports/annotations/multisports_det_anno.json\n",
    "!tree data/multisports/annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tools/generate_rgb.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Modify config file\n",
    "\n",
    "We use faster-rcnn_x101-64x4d_fpn_1x_coco as the base configuration, and make the following modifications to train on the MultiSports dataset. The following parts need to be modified:\n",
    "- Number of model categories\n",
    "- Learning rate adjustment strategy\n",
    "- Optimizer configuration\n",
    "- Dataset/annotation file path\n",
    "- Evaluator configuration\n",
    "- Pre-trained model \n",
    "  \n",
    "For more detailed tutorials, please refer to the [prepare configuration file](https://mmdetection.readthedocs.io/zh_CN/latest/user_guides/train.html#id9) document provided by MMDetection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) OpenMMLab. All rights reserved.\n",
      "_base_ = './faster-rcnn_r50-caffe_fpn_ms-1x_coco.py'\n",
      "model = dict(roi_head=dict(bbox_head=dict(num_classes=1)))\n",
      "\n",
      "# take 2 epochs as an example\n",
      "train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=2, val_interval=1)\n",
      "\n",
      "# learning rate\n",
      "param_scheduler = [\n",
      "    dict(type='ConstantLR', factor=1.0, by_epoch=False, begin=0, end=500)\n",
      "]\n",
      "\n",
      "# optimizer\n",
      "optim_wrapper = dict(\n",
      "    type='OptimWrapper',\n",
      "    optimizer=dict(type='SGD', lr=0.0050, momentum=0.9, weight_decay=0.0001))\n",
      "\n",
      "dataset_type = 'CocoDataset'\n",
      "# modify metainfo\n",
      "metainfo = {\n",
      "    'classes': ('person', ),\n",
      "    'palette': [\n",
      "        (220, 20, 60),\n",
      "    ]\n",
      "}\n",
      "\n",
      "# specify metainfo, dataset path\n",
      "data_root = 'data/multisports/'\n",
      "\n",
      "train_dataloader = dict(\n",
      "    dataset=dict(\n",
      "        data_root=data_root,\n",
      "        ann_file='annotations/multisports_det_anno_train.json',\n",
      "        data_prefix=dict(img='rawframes/'),\n",
      "        metainfo=metainfo))\n",
      "\n",
      "val_dataloader = dict(\n",
      "    dataset=dict(\n",
      "        data_root=data_root,\n",
      "        ann_file='annotations/multisports_det_anno_val.json',\n",
      "        data_prefix=dict(img='rawframes/'),\n",
      "        metainfo=metainfo))\n",
      "\n",
      "test_dataloader = dict(\n",
      "    dataset=dict(\n",
      "        data_root=data_root,\n",
      "        ann_file='annotations/ms_infer_anno.json',\n",
      "        data_prefix=dict(img='rawframes/'),\n",
      "        metainfo=metainfo))\n",
      "\n",
      "# specify annotaition file path, modify metric items\n",
      "val_evaluator = dict(\n",
      "    ann_file='data/multisports/annotations/multisports_det_anno_val.json',\n",
      "    metric_items=['mAP_50', 'AR@100'],\n",
      "    iou_thrs=[0.5],\n",
      ")\n",
      "\n",
      "test_evaluator = dict(\n",
      "    ann_file='data/multisports/annotations/ms_infer_anno.json',\n",
      "    metric_items=['mAP_50', 'AR@100'],\n",
      "    iou_thrs=[0.5],\n",
      ")\n",
      "\n",
      "# specify pretrain checkpoint\n",
      "load_from = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person/faster_rcnn_r50_fpn_1x_coco-person_20201216_175929-d022e227.pth'  # noqa: E501\n"
     ]
    }
   ],
   "source": [
    "!cat configs/faster-rcnn_r50-caffe_fpn_ms-1x_coco_ms_person.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train detection model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using MIM, you can directly train MMDetection models in the current directory. Here is the simplest example of training on a single GPU. For more training commands, please refer to the MIM [tutorial](https://github.com/open-mmlab/mim#command)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training command is /home/PJLAB/lilin/miniconda3/envs/ipy_stad/bin/python /home/PJLAB/lilin/miniconda3/envs/ipy_stad/lib/python3.9/site-packages/mmdet/.mim/tools/train.py configs/faster-rcnn_r50-caffe_fpn_ms-1x_coco_ms_person.py --launcher none --work-dir work_dirs/det_model. \n",
      "06/14 21:36:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 874241228\n",
      "    GPU 0: NVIDIA GeForce GTX 1660 Ti\n",
      "    CUDA_HOME: /usr/local/cuda-11.1\n",
      "    NVCC: Cuda compilation tools, release 11.1, V11.1.105\n",
      "    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
      "    PyTorch: 1.10.1+cu111\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX512\n",
      "  - CUDA Runtime 11.1\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.0.5\n",
      "  - Magma 2.5.2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
      "\n",
      "    TorchVision: 0.11.2+cu111\n",
      "    OpenCV: 4.7.0\n",
      "    MMEngine: 0.7.2\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: None\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "06/14 21:36:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "model = dict(\n",
      "    type='FasterRCNN',\n",
      "    data_preprocessor=dict(\n",
      "        type='DetDataPreprocessor',\n",
      "        mean=[103.53, 116.28, 123.675],\n",
      "        std=[1.0, 1.0, 1.0],\n",
      "        bgr_to_rgb=False,\n",
      "        pad_size_divisor=32),\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        depth=50,\n",
      "        num_stages=4,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        frozen_stages=1,\n",
      "        norm_cfg=dict(type='BN', requires_grad=False),\n",
      "        norm_eval=True,\n",
      "        style='caffe',\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained',\n",
      "            checkpoint='open-mmlab://detectron2/resnet50_caffe')),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[256, 512, 1024, 2048],\n",
      "        out_channels=256,\n",
      "        num_outs=5),\n",
      "    rpn_head=dict(\n",
      "        type='RPNHead',\n",
      "        in_channels=256,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            scales=[8],\n",
      "            ratios=[0.5, 1.0, 2.0],\n",
      "            strides=[4, 8, 16, 32, 64]),\n",
      "        bbox_coder=dict(\n",
      "            type='DeltaXYWHBBoxCoder',\n",
      "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "        loss_cls=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "    roi_head=dict(\n",
      "        type='StandardRoIHead',\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=dict(\n",
      "            type='Shared2FCBBoxHead',\n",
      "            in_channels=256,\n",
      "            fc_out_channels=1024,\n",
      "            roi_feat_size=7,\n",
      "            num_classes=1,\n",
      "            bbox_coder=dict(\n",
      "                type='DeltaXYWHBBoxCoder',\n",
      "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "            reg_class_agnostic=False,\n",
      "            loss_cls=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n",
      "            loss_bbox=dict(type='L1Loss', loss_weight=1.0))),\n",
      "    train_cfg=dict(\n",
      "        rpn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.7,\n",
      "                neg_iou_thr=0.3,\n",
      "                min_pos_iou=0.3,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=False),\n",
      "            allowed_border=-1,\n",
      "            pos_weight=-1,\n",
      "            debug=False),\n",
      "        rpn_proposal=dict(\n",
      "            nms_pre=2000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                min_pos_iou=0.5,\n",
      "                match_low_quality=False,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=True),\n",
      "            pos_weight=-1,\n",
      "            debug=False)),\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_pre=1000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            score_thr=0.05,\n",
      "            nms=dict(type='nms', iou_threshold=0.5),\n",
      "            max_per_img=100)))\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = 'data/multisports/'\n",
      "backend_args = None\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile', backend_args=None),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        type='RandomChoiceResize',\n",
      "        scales=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n",
      "                (1333, 768), (1333, 800)],\n",
      "        keep_ratio=True),\n",
      "    dict(type='RandomFlip', prob=0.5),\n",
      "    dict(type='PackDetInputs')\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile', backend_args=None),\n",
      "    dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        type='PackDetInputs',\n",
      "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n",
      "                   'scale_factor'))\n",
      "]\n",
      "train_dataloader = dict(\n",
      "    batch_size=2,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
      "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
      "    dataset=dict(\n",
      "        type='CocoDataset',\n",
      "        data_root='data/multisports/',\n",
      "        ann_file='annotations/multisports_det_anno_train.json',\n",
      "        data_prefix=dict(img='rawframes/'),\n",
      "        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', backend_args=None),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                type='RandomChoiceResize',\n",
      "                scales=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n",
      "                        (1333, 768), (1333, 800)],\n",
      "                keep_ratio=True),\n",
      "            dict(type='RandomFlip', prob=0.5),\n",
      "            dict(type='PackDetInputs')\n",
      "        ],\n",
      "        backend_args=None,\n",
      "        metainfo=dict(classes=('person', ), palette=[(220, 20, 60)])))\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    drop_last=False,\n",
      "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
      "    dataset=dict(\n",
      "        type='CocoDataset',\n",
      "        data_root='data/multisports/',\n",
      "        ann_file='annotations/multisports_det_anno_val.json',\n",
      "        data_prefix=dict(img='rawframes/'),\n",
      "        test_mode=True,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', backend_args=None),\n",
      "            dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                type='PackDetInputs',\n",
      "                meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n",
      "                           'scale_factor'))\n",
      "        ],\n",
      "        backend_args=None,\n",
      "        metainfo=dict(classes=('person', ), palette=[(220, 20, 60)])))\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    drop_last=False,\n",
      "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
      "    dataset=dict(\n",
      "        type='CocoDataset',\n",
      "        data_root='data/multisports/',\n",
      "        ann_file='annotations/ms_infer_anno.json',\n",
      "        data_prefix=dict(img='rawframes/'),\n",
      "        test_mode=True,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', backend_args=None),\n",
      "            dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                type='PackDetInputs',\n",
      "                meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n",
      "                           'scale_factor'))\n",
      "        ],\n",
      "        backend_args=None,\n",
      "        metainfo=dict(classes=('person', ), palette=[(220, 20, 60)])))\n",
      "val_evaluator = dict(\n",
      "    type='CocoMetric',\n",
      "    ann_file='data/multisports/annotations/multisports_det_anno_val.json',\n",
      "    metric='bbox',\n",
      "    format_only=False,\n",
      "    backend_args=None,\n",
      "    metric_items=['mAP_50', 'AR@100'],\n",
      "    iou_thrs=[0.5])\n",
      "test_evaluator = dict(\n",
      "    type='CocoMetric',\n",
      "    ann_file='data/multisports/annotations/ms_infer_anno.json',\n",
      "    metric='bbox',\n",
      "    format_only=False,\n",
      "    backend_args=None,\n",
      "    metric_items=['mAP_50', 'AR@100'],\n",
      "    iou_thrs=[0.5])\n",
      "train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=2, val_interval=1)\n",
      "val_cfg = dict(type='ValLoop')\n",
      "test_cfg = dict(type='TestLoop')\n",
      "param_scheduler = [\n",
      "    dict(type='ConstantLR', factor=1.0, by_epoch=False, begin=0, end=500)\n",
      "]\n",
      "optim_wrapper = dict(\n",
      "    type='OptimWrapper',\n",
      "    optimizer=dict(type='SGD', lr=0.005, momentum=0.9, weight_decay=0.0001))\n",
      "auto_scale_lr = dict(enable=False, base_batch_size=16)\n",
      "default_scope = 'mmdet'\n",
      "default_hooks = dict(\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    logger=dict(type='LoggerHook', interval=50),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    checkpoint=dict(type='CheckpointHook', interval=1),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    visualization=dict(type='DetVisualizationHook'))\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
      "    dist_cfg=dict(backend='nccl'))\n",
      "vis_backends = [dict(type='LocalVisBackend')]\n",
      "visualizer = dict(\n",
      "    type='DetLocalVisualizer',\n",
      "    vis_backends=[dict(type='LocalVisBackend')],\n",
      "    name='visualizer')\n",
      "log_processor = dict(type='LogProcessor', window_size=50, by_epoch=True)\n",
      "log_level = 'INFO'\n",
      "load_from = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person/faster_rcnn_r50_fpn_1x_coco-person_20201216_175929-d022e227.pth'\n",
      "resume = False\n",
      "metainfo = dict(classes=('person', ), palette=[(220, 20, 60)])\n",
      "launcher = 'none'\n",
      "work_dir = 'work_dirs/det_model'\n",
      "\n",
      "06/14 21:36:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "06/14 21:36:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "06/14 21:36:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - load model from: open-mmlab://detectron2/resnet50_caffe\n",
      "06/14 21:36:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by openmmlab backend from path: open-mmlab://detectron2/resnet50_caffe\n",
      "06/14 21:36:40 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: conv1.bias\n",
      "\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person/faster_rcnn_r50_fpn_1x_coco-person_20201216_175929-d022e227.pth\n",
      "06/14 21:36:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person/faster_rcnn_r50_fpn_1x_coco-person_20201216_175929-d022e227.pth\n",
      "06/14 21:36:40 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "06/14 21:36:40 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "06/14 21:36:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /home/PJLAB/lilin/Repos/mmact_dev/mmaction2/projects/stad_tutorial/work_dirs/det_model.\n",
      "06/14 21:37:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 50/118]  lr: 5.0000e-03  eta: 0:01:36  time: 0.5172  data_time: 0.0048  memory: 3509  loss: 0.5597  loss_rpn_cls: 0.0067  loss_rpn_bbox: 0.0171  loss_cls: 0.1985  acc: 95.8984  loss_bbox: 0.3374\n",
      "06/14 21:37:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][100/118]  lr: 5.0000e-03  eta: 0:01:10  time: 0.5207  data_time: 0.0024  memory: 3509  loss: 0.3666  loss_rpn_cls: 0.0019  loss_rpn_bbox: 0.0131  loss_cls: 0.1267  acc: 94.8242  loss_bbox: 0.2248\n",
      "06/14 21:37:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: faster-rcnn_r50-caffe_fpn_ms-1x_coco_ms_person_20230614_213636\n",
      "06/14 21:37:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "06/14 21:37:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 50/120]    eta: 0:00:07  time: 0.1084  data_time: 0.0031  memory: 3509  \n",
      "06/14 21:37:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][100/120]    eta: 0:00:02  time: 0.1063  data_time: 0.0012  memory: 824  \n",
      "06/14 21:37:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area=   all | maxDets=100 ] = 0.895\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area= small | maxDets=1000 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area=medium | maxDets=1000 ] = 0.670\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area= large | maxDets=1000 ] = 0.901\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=100 ] = 0.953\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=300 ] = 0.953\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=1000 ] = 0.953\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area= small | maxDets=1000 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=medium | maxDets=1000 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area= large | maxDets=1000 ] = 0.952\n",
      "06/14 21:37:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.895 -1.000 -1.000 -1.000 0.670 0.901\n",
      "06/14 21:37:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][120/120]    coco/bbox_mAP_50: -1.0000  coco/bbox_AR@100: 0.9530  data_time: 0.0012  time: 0.1065\n",
      "06/14 21:38:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 50/118]  lr: 5.0000e-03  eta: 0:00:35  time: 0.5167  data_time: 0.0026  memory: 3509  loss: 0.3253  loss_rpn_cls: 0.0019  loss_rpn_bbox: 0.0116  loss_cls: 0.1067  acc: 96.7773  loss_bbox: 0.2052\n",
      "06/14 21:38:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][100/118]  lr: 5.0000e-03  eta: 0:00:09  time: 0.5214  data_time: 0.0022  memory: 3509  loss: 0.3155  loss_rpn_cls: 0.0013  loss_rpn_bbox: 0.0103  loss_cls: 0.1025  acc: 92.4805  loss_bbox: 0.2013\n",
      "06/14 21:38:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: faster-rcnn_r50-caffe_fpn_ms-1x_coco_ms_person_20230614_213636\n",
      "06/14 21:38:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "06/14 21:39:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 50/120]    eta: 0:00:07  time: 0.1074  data_time: 0.0014  memory: 3509  \n",
      "06/14 21:39:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][100/120]    eta: 0:00:02  time: 0.1070  data_time: 0.0012  memory: 824  \n",
      "06/14 21:39:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area=   all | maxDets=100 ] = 0.919\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area= small | maxDets=1000 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area=medium | maxDets=1000 ] = 0.734\n",
      " Average Precision  (AP) @[ IoU=0.50:0.50 | area= large | maxDets=1000 ] = 0.922\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=100 ] = 0.955\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=300 ] = 0.955\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=   all | maxDets=1000 ] = 0.955\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area= small | maxDets=1000 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area=medium | maxDets=1000 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.50 | area= large | maxDets=1000 ] = 0.954\n",
      "06/14 21:39:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.919 -1.000 -1.000 -1.000 0.734 0.922\n",
      "06/14 21:39:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][120/120]    coco/bbox_mAP_50: -1.0000  coco/bbox_AR@100: 0.9550  data_time: 0.0012  time: 0.1072\n",
      "\u001b[32mTraining finished successfully. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!mim train mmdet configs/faster-rcnn_r50-caffe_fpn_ms-1x_coco_ms_person.py \\\n",
    "    --work-dir work_dirs/det_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Generating Proposal BBoxes\n",
    "\n",
    "During the training of the spatiotemporal action detection model, we need to rely on proposals generated by the detection model, rather than annotated detection boxes. Therefore, we need to use a trained detection model to perform inference on the entire dataset and convert the resulting proposals into the required format for subsequent training.\n",
    "\n",
    "#### 2.4.1 Converting the Dataset to Coco Format\n",
    "\n",
    "We provide a script to convert the MultiSports dataset into an annotation format without ground truth, which is used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo 'person' > data/multisports/annotations/label_map.txt\n",
    "!python tools/images2coco.py \\\n",
    "        data/multisports/rawframes \\\n",
    "        data/multisports/annotations/label_map.txt \\\n",
    "        ms_infer_anno.json "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Inference for Generating Proposal Files\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference of MMDetection models is also based on MIM. For more testing commands, please refer to the MIM [tutorial](GitHub - open-mmlab/mim: MIM Installs OpenMMLab Packages).\n",
    "\n",
    "After the inference is completed, the results will be saved in 'data/multisports/ms_proposals.pkl'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mim test mmdet configs/faster-rcnn_r50-caffe_fpn_ms-1x_coco_ms_person.py \\\n",
    "    --checkpoint work_dirs/det_model/epoch_2.pth \\\n",
    "    --out data/multisports/annotations/ms_det_proposals.pkl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Spatio-temporal Action Detection Model\n",
    "The provided annotation files and the proposal files generated by MMDetection need to be converted to the required format for training the spatiotemporal action detection model. We have provided relevant script to generate the specified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert annotation files\n",
    "!python ../../tools/data/multisports/parse_anno.py \n",
    "\n",
    "# Convert proposal files\n",
    "!python tools/convert_proposals.py \n",
    "\n",
    "!tree data/multisports/annotations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training the Spatio-temporal Action Detection Model\n",
    "\n",
    "MMAction2 already supports training on the MultiSports dataset. You just need to modify the path to the proposal file. For detailed configurations, please refer to the [config](configs/slowonly_k400_multisports.py) file. Since the training data is limited, the configuration uses a pre-trained model trained on the complete MultiSports dataset. When training with a custom dataset, you don't need to specify the `load_from` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using MIM\n",
    "!mim train mmaction2 configs/slowonly_k400_multisports.py \\\n",
    "    --work-dir work_dirs/stad_model/ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inferring the Spatiotemporal Action Detection Model\n",
    "\n",
    "After training the detection model and the spatiotemporal action detection model, we can use the spatiotemporal action detection demo for inference and visualize the model's performance.\n",
    "\n",
    "Since the tutorial uses a limited training dataset, the model's performance is not optimal, so a pre-trained model is used for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../../demo/demo_spatiotemporal_det.py \\\n",
    "    data/multisports/test/aerobic_gymnastics/v_7G_IpU0FxLU_c001.mp4 \\\n",
    "    data/demo_spatiotemporal_det.mp4 \\\n",
    "    --config configs/slowonly_k400_multisports.py \\\n",
    "    --checkpoint https://download.openmmlab.com/mmaction/v1.0/detection/slowonly/slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-8e_multisports-rgb/slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-8e_multisports-rgb_20230320-a1ca5e76.pth \\\n",
    "    --det-config configs/faster-rcnn_r50-caffe_fpn_ms-1x_coco_ms_person.py \\\n",
    "    --det-checkpoint work_dirs/det_model/epoch_2.pth \\\n",
    "    --det-score-thr 0.85 \\\n",
    "    --action-score-thr 0.8 \\\n",
    "    --label-map ../../tools/data/multisports/label_map.txt \\\n",
    "    --predict-stepsize 8 \\\n",
    "    --output-stepsize 1 \\\n",
    "    --output-fps 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Video\n",
    "import moviepy.editor\n",
    "moviepy.editor.ipython_display(\"data/demo_spatiotemporal_det.mp4\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ipy_stad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
